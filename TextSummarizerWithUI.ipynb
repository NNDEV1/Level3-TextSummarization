{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarizerWithUI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH5gelLicUEl",
        "outputId": "118b593f-95ac-4215-be7b-00d23d635799"
      },
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "print(gr.__version__)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmv77qEbcWuF",
        "outputId": "eb26fc92-990f-4b64-e04a-6128d1f0a3cd"
      },
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install allennlp==1.0.0 allennlp-models==1.0.0 --quiet\n",
        "#!pip install --user gradio\n",
        "\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import nltk\n",
        "import heapq\n",
        "from transformers import pipeline\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qt_YCztmaPf"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "summarizer = pipeline(\"summarization\") # model=\"t5-small\"\n",
        "# predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz\")\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1RAhsV_cYbn"
      },
      "source": [
        "def url_validator(x):\n",
        "\n",
        "    try:\n",
        "\n",
        "        result = urlparse(x)\n",
        "        return all([result.scheme, result.netloc])\n",
        "\n",
        "    except:\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "#First compile everything into a function\n",
        "def main(url_or_plain_text, question, bullet_points=5):\n",
        "\n",
        "    if url_validator(url_or_plain_text):\n",
        "\n",
        "        url = url_or_plain_text\n",
        "        TotalContent = \"\"\n",
        "\n",
        "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n"
        "        htmlDoc = urlopen(req).read()\n",
        "\n",
        "        soupObj = bs(htmlDoc, 'html.parser')\n",
        "        paragraphContents = soupObj.findAll('p')\n",
        "\n",
        "        for content in paragraphContents:\n",
        "\n",
        "            TotalContent += content.text\n",
        "\n",
        "    else:\n",
        "\n",
        "        TotalContent = url_or_plain_text\n",
        "\n",
        "\n",
        "    total_cleaned_content = re.sub(r'\\[[0-9]*\\]', ' ', TotalContent)\n",
        "    total_cleaned_content = re.sub(r'\\s+', ' ', total_cleaned_content)\n",
        "\n",
        "    sentence_tokens = nltk.sent_tokenize(total_cleaned_content)\n",
        "\n",
        "    total_cleaned_content = re.sub(r'[^a-zA-Z]', ' ', total_cleaned_content)\n",
        "    total_cleaned_content = re.sub(r'\\s+', ' ', total_cleaned_content)\n",
        "\n",
        "    #print(sentence_tokens)\n",
        "\n",
        "    words_tokens = nltk.word_tokenize(total_cleaned_content)\n",
        "\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    word_frequencies = {}\n",
        "\n",
        "    for word in words_tokens:\n",
        "\n",
        "        if word not in stopwords:\n",
        "\n",
        "            if word not in word_frequencies.keys():\n",
        "\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "\n",
        "                word_frequencies[word] += 1\n",
        "\n",
        "\n",
        "    #print(words_tokens)\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        \n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
        "\n",
        "    #print(word_frequencies)\n",
        "\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for sentence in sentence_tokens:\n",
        "\n",
        "        for word in nltk.word_tokenize(sentence.lower()):\n",
        "\n",
        "            if word in word_frequencies.keys():\n",
        "\n",
        "                if (len(sentence.split(' '))) < 30:\n",
        "\n",
        "                    if sentence not in sentence_scores.keys():\n",
        "                        sentence_scores[sentence] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "\n",
        "    points = heapq.nlargest(int(len(sentence_tokens)/2), sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    total_text = \"\"\n",
        "\n",
        "    for i in points:\n",
        "\n",
        "        total_text += i\n",
        "\n",
        "    bullet_point_string = \"\"\"\"\"\"\n",
        "\n",
        "    for i in range(bullet_points):\n",
        "        \n",
        "        bullet_point_string += \"\\n - \" + points[i]\n",
        "\n",
        "    \n",
        "\n",
        "    summary = summarizer(total_text, max_length=128, min_length=64, do_sample=False)\n",
        "\n",
        "    result=predictor.predict(\n",
        "    passage=TotalContent,\n",
        "    question=question\n",
        "    )\n",
        "\n",
        "    return summary[0][\"summary_text\"], bullet_point_string, result['best_span_str']\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "FNJFV9GZizip",
        "outputId": "f0c11bdf-77af-4696-e205-8f27689d92c1"
      },
      "source": [
        "gr.Interface(\n",
        "  fn=main, \n",
        "  inputs=[gr.inputs.Textbox(lines=5, placeholder=\"A text consisting of 10 or more sentences is recommended\"), \n",
        "          gr.inputs.Textbox(lines=2), \n",
        "          gr.inputs.Slider(0, 10, step=1)],\n",
        "  outputs=[\"text\", \"text\", \"text\"], \n",
        "  theme=\"huggingface\").launch(debug=False, inbrowser=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://29328.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://29328.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f3d09f24f90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7868/',\n",
              " 'https://29328.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG3y_MuZkwGH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
