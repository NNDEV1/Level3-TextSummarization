# -*- coding: utf-8 -*-
"""Level3TextSummarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13qRmrw7cpYIfk_mldXPwOtUVz_BPY6rT
"""

#!pip install transformers --quiet
#!pip install simpletransformers --quiet
#!pip install allennlp==1.0.0 allennlp-models==1.0.0 --quiet

from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq
from transformers import pipeline


nltk.download('punkt')
nltk.download('stopwords')

url = 'https://en.wikipedia.org/wiki/Olympic_Games'
TotalContent = ""

htmlDoc = request.urlopen(url)

soupObj = bs(htmlDoc, 'html.parser')
paragraphContents = soupObj.findAll('p')

for content in paragraphContents:

  TotalContent += content.text



total_cleaned_content = re.sub(r'\[[0-9]*\]', ' ', TotalContent)
total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

sentence_tokens = nltk.sent_tokenize(total_cleaned_content)

total_cleaned_content = re.sub(r'[^a-zA-Z]', ' ', total_cleaned_content)
total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

print(sentence_tokens)

words_tokens = nltk.word_tokenize(total_cleaned_content)

stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}

for word in words_tokens:

  if word not in stopwords:

    if word not in word_frequencies.keys():

      word_frequencies[word] = 1
    else:

      word_frequencies[word] += 1


print(words_tokens)

maximum_frequency = max(word_frequencies.values())

for word in word_frequencies.keys():
  word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

print(word_frequencies)

sentence_scores = {}

for sentence in sentence_tokens:

  for word in nltk.word_tokenize(sentence.lower()):

    if word in word_frequencies.keys():

      if (len(sentence.split(' '))) < 30:

        if sentence not in sentence_scores.keys():
          sentence_scores[sentence] = word_frequencies[word]
        else:
          sentence_scores[sentence] += word_frequencies[word]


points = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)

total_text = ""

for i in points:

    total_text += i

summarizer = pipeline("summarization") # model="t5-small"

summary = summarizer(total_text, max_length=128, min_length=64, do_sample=False)

#summary[0]["summary_text"]

from allennlp.predictors.predictor import Predictor
## Old Model commented

# predictor = Predictor.from_path("https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz")
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz")

result=predictor.predict(
  passage=TotalContent,
  question= "how many nations participating in the olympics?"
)
result['best_span_str']

print("Bullet Points: ")

for i in points:
    
   print("\n - " + i)

print("\n Summary: " + str(summary[0]["summary_text"]))

