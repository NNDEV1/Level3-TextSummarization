# -*- coding: utf-8 -*-
"""TextSummarizerWithUI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nIahtDqCXsXLHvhDacYz9dUldz9Fof4o
"""

#!pip install gradio --quiet

import gradio as gr
import time

print(gr.__version__)

#!pip install transformers --quiet
#!pip install allennlp==1.0.0 allennlp-models==1.0.0 --quiet


from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq
from transformers import pipeline

nltk.download('punkt')
nltk.download('stopwords')

from allennlp.predictors.predictor import Predictor

summarizer = pipeline("summarization") # model="t5-small"
# predictor = Predictor.from_path("https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz")
predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz")

#First compile everything into a function
def main(website, question, bullet_points):

    url = website
    TotalContent = ""

    htmlDoc = request.urlopen(url)

    soupObj = bs(htmlDoc, 'html.parser')
    paragraphContents = soupObj.findAll('p')

    for content in paragraphContents:

        TotalContent += content.text



    total_cleaned_content = re.sub(r'\[[0-9]*\]', ' ', TotalContent)
    total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

    sentence_tokens = nltk.sent_tokenize(total_cleaned_content)

    total_cleaned_content = re.sub(r'[^a-zA-Z]', ' ', total_cleaned_content)
    total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

    #print(sentence_tokens)

    words_tokens = nltk.word_tokenize(total_cleaned_content)

    stopwords = nltk.corpus.stopwords.words('english')

    word_frequencies = {}

    for word in words_tokens:

        if word not in stopwords:

            if word not in word_frequencies.keys():

                word_frequencies[word] = 1
            else:

                word_frequencies[word] += 1


    #print(words_tokens)

    maximum_frequency = max(word_frequencies.values())

    for word in word_frequencies.keys():
        
        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

    #print(word_frequencies)

    sentence_scores = {}

    for sentence in sentence_tokens:

        for word in nltk.word_tokenize(sentence.lower()):

            if word in word_frequencies.keys():

                if (len(sentence.split(' '))) < 30:

                    if sentence not in sentence_scores.keys():
                        sentence_scores[sentence] = word_frequencies[word]
                    else:
                        sentence_scores[sentence] += word_frequencies[word]


    points = heapq.nlargest(20, sentence_scores, key=sentence_scores.get)

    total_text = ""

    for i in points:

        total_text += i

    string = """"""

    for i in range(bullet_points):
        
        string += "\n - " + points[i]

    

    summary = summarizer(total_text, max_length=128, min_length=64, do_sample=False)

    result=predictor.predict(
    passage=TotalContent,
    question=question
    )

    return summary[0]["summary_text"], string, result['best_span_str']

gr.Interface(
  fn=main, 
  inputs=["text", "text", gr.inputs.Slider(0, 20, step=1)],
  outputs=["text", "text", "text"]).launch(debug=False)

